{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up Google Colab and Hugging Face API\n",
    "\n",
    "## Setting up Google Colab\n",
    "1. **Open this notebook**: go to [Colab](https://colab.research.google.com/github/Chair-of-Banking-and-Finance/Bachelor_thesis_24_25_Template/blob/main/Llama_RAG/llama2%20notebook.ipynb).\n",
    "2. **Connect to runtime**: Click on the \"Connect\" button at the top right corner of the screen. This will connect your notebook to a virtual machine with GPU support.\n",
    "3. **Set GPU as hardware accelerator**: Go to \"Runtime\" -> \"Change runtime type\" and select \"GPU\" from the hardware accelerator dropdown menu. This will ensure your notebook is using a GPU, which is highly recommended for working with large language models like Llama 2.\n",
    "4. **Install required packages**: Run the installation commands provided in the script to install all necessary dependencies for the project. These include `torch`, `transformers`, `langchain`, and others.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zow6C3MdRQGX"
   },
   "source": [
    "# Every part containing a flag ðŸš© at the start of the code may need some input of you\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GEkCVJYLEXdV"
   },
   "source": [
    "# Following part will install and import the necessary libraries on your notebook\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a49-1vcdA9-6"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "HEPOoxAOEidC",
    "outputId": "485d03dc-77b4-44d7-c75b-be8ef5c2c8e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.3.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
      "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
      "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
      "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
      "Hit:4 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
      "Ign:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
      "Hit:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
      "Hit:7 https://r2u.stat.illinois.edu/ubuntu jammy Release\n",
      "Hit:8 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
      "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
      "Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
      "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
      "Reading package lists... Done\n",
      "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 53 not upgraded.\n",
      "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.10/dist-packages (1.8.0.post1)\n",
      "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
      "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (1.0.1)\n",
      "Requirement already satisfied: replicate in /usr/local/lib/python3.10/dist-packages (0.32.1)\n",
      "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (9.4.0)\n",
      "Requirement already satisfied: pytesseract in /usr/local/lib/python3.10/dist-packages (0.3.13)\n",
      "Requirement already satisfied: numpy<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.44.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.5)\n",
      "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.4.0+cu121)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.3.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.24.6)\n",
      "Requirement already satisfied: httpx<1,>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from replicate) (0.27.2)\n",
      "Requirement already satisfied: pydantic>1.10.7 in /usr/local/lib/python3.10/dist-packages (from replicate) (2.8.2)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from replicate) (4.12.2)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.21.0->replicate) (3.7.1)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.21.0->replicate) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.21.0->replicate) (1.0.5)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.21.0->replicate) (3.8)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.21.0->replicate) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.21.0->replicate) (0.14.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.15.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2024.6.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>1.10.7->replicate) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>1.10.7->replicate) (2.20.1)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2024.5.15)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.4.4)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.19.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.21.0->replicate) (1.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.0.7)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "# Install necessary libraries\n",
    "!pip install scikit-learn\n",
    "!apt-get update\n",
    "!apt-get install -y tesseract-ocr\n",
    "!pip install faiss-cpu sentence-transformers python-dotenv replicate PyPDF2 Pillow pytesseract\n",
    "\n",
    "# Import necessary libraries\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import os\n",
    "import replicate\n",
    "import PyPDF2\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FrAGb39GE02a"
   },
   "source": [
    "# ðŸš© Run this code once you inserted your Replicate API Token into the \"YOUR_API_TOKEN\" part. You can get it here https://replicate.com/account/api-tokens - after a few 100 invoices the billing is 0.03$ for me. You can set up your own monthly billing limit aswell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m04HJDHQFyLi"
   },
   "outputs": [],
   "source": [
    "# Set the API token directly in the environment\n",
    "os.environ[\"REPLICATE_API_TOKEN\"] = \"HERE YOU PUT IN YOUR API TOKEN\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g3XCG_9tGIEk"
   },
   "source": [
    "# ðŸš© Creation of definition of file path, embedding, context search and respond generation and sentence transformer model -> Change chunk size in line 38 by increasing or decreasing it, which may resolve to more accurate reponses. E.g. instead of 256 you can type following: sentences = [text[i:i+512] for i in range(0, len(text), 512)]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iJVNS-kJGUMS",
    "outputId": "4878ece1-2162-4da7-f075-e351b6957d41"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def read_file(file_path):\n",
    "    \"\"\"\n",
    "    Reads a PDF or text file and extracts text from it, including OCR for images.\n",
    "    \"\"\"\n",
    "    text = \"\"\n",
    "    try:\n",
    "        if file_path.lower().endswith('.pdf'):\n",
    "            reader = PyPDF2.PdfReader(file_path)\n",
    "            for page_num in range(len(reader.pages)):\n",
    "                page = reader.pages[page_num]\n",
    "                text += page.extract_text() or \"\"\n",
    "\n",
    "                # Check for images in the page\n",
    "                resources = page.get('/Resources')\n",
    "                if resources:\n",
    "                    x_objects = resources.get('/XObject')\n",
    "                    if isinstance(x_objects, PyPDF2.generic.DictionaryObject):\n",
    "                        for obj in x_objects:\n",
    "                            x_object = x_objects[obj]\n",
    "                            if isinstance(x_object, PyPDF2.generic.IndirectObject):\n",
    "                                x_object = x_object.get_object()  # Dereference the IndirectObject\n",
    "                            if x_object.get('/Subtype') == '/Image':\n",
    "                                try:\n",
    "                                    # Extract image data\n",
    "                                    data = x_object._data  # Access raw data\n",
    "                                    # Check image filter type\n",
    "                                    if '/Filter' in x_object:\n",
    "                                        if x_object['/Filter'] == '/DCTDecode':  # JPEG format\n",
    "                                            img = Image.open(io.BytesIO(data))\n",
    "                                        elif x_object['/Filter'] == '/JPXDecode':  # JPEG2000 format\n",
    "                                            img = Image.open(io.BytesIO(data))\n",
    "                                        elif x_object['/Filter'] == '/FlateDecode':  # PNG format\n",
    "                                            img = Image.open(io.BytesIO(data))\n",
    "                                        else:\n",
    "                                            raise ValueError(f\"Unsupported image format: {x_object['/Filter']}\")\n",
    "                                    text += pytesseract.image_to_string(img)\n",
    "                                except Exception as img_error:\n",
    "                                    print(f\"Error processing image on page {page_num}: {img_error}\")\n",
    "                    else:\n",
    "                        print(f\"Page {page_num}: No images or unsupported format\")\n",
    "        elif file_path.lower().endswith('.txt'):\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported file format. Only PDF and TXT files are supported.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file: {e}\")\n",
    "    return text\n",
    "\n",
    "def read_files(file_paths):\n",
    "    \"\"\"\n",
    "    Reads multiple PDF or text files and combines their text content.\n",
    "    \"\"\"\n",
    "    combined_text = \"\"\n",
    "    for file_path in file_paths:\n",
    "        combined_text += read_file(file_path) + \"\\n\"\n",
    "    return combined_text\n",
    "\n",
    "def create_embeddings(text, model):\n",
    "    \"\"\"\n",
    "    Creates embeddings for the given text using the specified model.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Split text into sentences or smaller chunks\n",
    "        sentences = text.split('. ')\n",
    "        embeddings = model.encode(sentences)\n",
    "        return sentences, embeddings\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating embeddings: {e}\")\n",
    "        return [], np.array([])\n",
    "\n",
    "def search_context(query, sentences, embeddings, model, top_k=5):\n",
    "    \"\"\"\n",
    "    Searches for relevant context for the given query using FAISS.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        query_embedding = model.encode([query])[0]\n",
    "        # Ensure the embeddings are in the correct shape\n",
    "        embeddings = np.array(embeddings).astype('float32')\n",
    "        index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "        index.add(embeddings)\n",
    "        D, I = index.search(np.array([query_embedding]), k=top_k)\n",
    "        return [sentences[i] for i in I[0]]\n",
    "    except Exception as e:\n",
    "        print(f\"Error searching for context: {e}\")\n",
    "        return []\n",
    "\n",
    "def calculate_cosine_similarity(query, embeddings, model):\n",
    "    \"\"\"\n",
    "    Calculates cosine similarity between the query and all sentences.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        query_embedding = model.encode([query])\n",
    "        similarities = cosine_similarity(query_embedding, embeddings)\n",
    "        return similarities[0]  # Return similarity scores for the query\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating cosine similarity: {e}\")\n",
    "        return []\n",
    "\n",
    "def generate_llama2_response(prompt_input, context, pre_prompt):\n",
    "    \"\"\"\n",
    "    Generates a response using the LLaMA2 model.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        prompt_with_context = f\"{pre_prompt}\\n\\nContext: {context}\\n\\nUser: {prompt_input}\\nAssistant: \"\n",
    "        output = replicate.run(\n",
    "            'a16z-infra/llama13b-v2-chat:df7690f1994d94e96ad9d568eac121aecf50684a0b0963b25a41cc40061269e5',\n",
    "            input={\"prompt\": prompt_with_context, \"temperature\": 0, \"top_p\": 1, \"max_length\": 128, \"repetition_penalty\": 1}\n",
    "        )\n",
    "        response = ''.join(output)\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating response: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Initialize the Sentence Transformer model\n",
    "sentence_model = SentenceTransformer('all-MiniLM-L6-v2')  # LLM being used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6xG3vxLcGbYW"
   },
   "source": [
    "# ðŸš© Definition of pre-prompts. Insert any pre-prompt possibly matching for your queries. There are already a few examples given. Feel free to change/add some in the given format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0XZMsZfbG5ZX"
   },
   "outputs": [],
   "source": [
    "# New pre-prompt\n",
    "pre_prompt = (\n",
    "    \"You are a specialized financial analysis assistant with expertise in interpreting and summarizing financial analyst reports found in the 'sample_data' folder. \"\n",
    "    \"Your task is to provide data-driven answers based on this document, ensuring that your responses are directly relevant to the user's queries. \"\n",
    "    \"Keep your responses short, concise, and organized into clear paragraphs to facilitate understanding. \"\n",
    "    \"Focus on delivering exactly as in the document given, precise and accurate financial insights extracted from the document.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ThkEeDkbHDHp"
   },
   "source": [
    "# Query handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kpy4XrU-HPud"
   },
   "outputs": [],
   "source": [
    "def handle_query(prompt_input):\n",
    "    \"\"\"\n",
    "    Handles user queries by retrieving relevant context and generating responses.\n",
    "    \"\"\"\n",
    "    context = search_context(prompt_input, sentences, embeddings, sentence_model, top_k=5)\n",
    "    context_text = \" \".join(context)\n",
    "    response = generate_llama2_response(prompt_input, context_text, pre_prompt)\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zl2QfcsLHYDP"
   },
   "source": [
    "# ðŸš© Insert your file from the \"/content/sample_data\" folder on the <- left side. Right\n",
    "\n",
    "---\n",
    "\n",
    "click on the document and click on copy path. Insert the path in the code here: file_path = \"YOUR_FILE_PATH\"\n",
    "\n",
    "# Example: file_path = \"/content/sample_data/JP Morgan BMW@GR BMW Q3â€™23 First Take strong quarter.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "t758_FnVIIjT",
    "outputId": "fffcb229-5686-4019-9126-707c22aaa1a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error reading file: 'IndirectObject' object has no attribute 'get'\n",
      "Page 0: No images or unsupported format\n",
      "Page 1: No images or unsupported format\n",
      "Page 2: No images or unsupported format\n",
      "Page 3: No images or unsupported format\n",
      "Page 4: No images or unsupported format\n",
      "Page 5: No images or unsupported format\n",
      "Page 6: No images or unsupported format\n",
      "Page 7: No images or unsupported format\n",
      "Page 8: No images or unsupported format\n",
      "Page 9: No images or unsupported format\n",
      "Page 10: No images or unsupported format\n",
      "Page 11: No images or unsupported format\n",
      "Page 12: No images or unsupported format\n",
      "Page 13: No images or unsupported format\n",
      "Page 14: No images or unsupported format\n",
      "Page 15: No images or unsupported format\n",
      "Page 16: No images or unsupported format\n",
      "Page 17: No images or unsupported format\n",
      "Page 18: No images or unsupported format\n",
      "Page 19: No images or unsupported format\n",
      "Page 20: No images or unsupported format\n",
      "Page 21: No images or unsupported format\n",
      "Page 22: No images or unsupported format\n",
      "Page 23: No images or unsupported format\n",
      "Page 24: No images or unsupported format\n",
      "Page 25: No images or unsupported format\n",
      "Page 26: No images or unsupported format\n",
      "Page 27: No images or unsupported format\n",
      "Page 28: No images or unsupported format\n",
      "Page 29: No images or unsupported format\n",
      "Page 30: No images or unsupported format\n",
      "Page 31: No images or unsupported format\n",
      "Page 32: No images or unsupported format\n",
      "Page 33: No images or unsupported format\n",
      "Page 34: No images or unsupported format\n",
      "Page 35: No images or unsupported format\n",
      "Page 36: No images or unsupported format\n",
      "Error processing image on page 37: cannot identify image file <_io.BytesIO object at 0x79715cccf5b0>\n",
      "Page 38: No images or unsupported format\n",
      "Error processing image on page 39: cannot identify image file <_io.BytesIO object at 0x79715cccf600>\n",
      "Page 40: No images or unsupported format\n",
      "Error processing image on page 41: cannot identify image file <_io.BytesIO object at 0x79715cccf150>\n",
      "Error processing image on page 42: cannot identify image file <_io.BytesIO object at 0x79715cccf010>\n",
      "Error processing image on page 43: cannot identify image file <_io.BytesIO object at 0x79715cccebb0>\n",
      "Page 44: No images or unsupported format\n",
      "Error processing image on page 45: cannot identify image file <_io.BytesIO object at 0x79715ccce660>\n",
      "Page 46: No images or unsupported format\n",
      "Error processing image on page 47: cannot identify image file <_io.BytesIO object at 0x79715ccceb60>\n",
      "Page 48: No images or unsupported format\n",
      "Page 49: No images or unsupported format\n",
      "Page 50: No images or unsupported format\n",
      "Page 51: No images or unsupported format\n",
      "Page 52: No images or unsupported format\n",
      "Page 53: No images or unsupported format\n",
      "Page 54: No images or unsupported format\n",
      "Page 55: No images or unsupported format\n",
      "Page 56: No images or unsupported format\n",
      "Page 57: No images or unsupported format\n",
      "Page 58: No images or unsupported format\n",
      "Page 59: No images or unsupported format\n",
      "Page 60: No images or unsupported format\n",
      "Page 61: No images or unsupported format\n",
      "Page 62: No images or unsupported format\n",
      "Page 63: No images or unsupported format\n",
      "Page 64: No images or unsupported format\n",
      "Page 65: No images or unsupported format\n",
      "Page 66: No images or unsupported format\n",
      "Page 67: No images or unsupported format\n",
      "Page 68: No images or unsupported format\n",
      "Page 69: No images or unsupported format\n",
      "Page 70: No images or unsupported format\n",
      "Page 71: No images or unsupported format\n",
      "Page 72: No images or unsupported format\n",
      "Page 73: No images or unsupported format\n",
      "Page 74: No images or unsupported format\n",
      "Page 75: No images or unsupported format\n",
      "Page 76: No images or unsupported format\n",
      "Page 77: No images or unsupported format\n",
      "Page 78: No images or unsupported format\n",
      "Page 79: No images or unsupported format\n",
      "Page 80: No images or unsupported format\n",
      "Page 81: No images or unsupported format\n",
      "Page 82: No images or unsupported format\n",
      "Page 83: No images or unsupported format\n",
      "Page 84: No images or unsupported format\n",
      "Page 85: No images or unsupported format\n",
      "Page 86: No images or unsupported format\n",
      "Page 87: No images or unsupported format\n",
      "Page 88: No images or unsupported format\n",
      "Page 89: No images or unsupported format\n",
      "Page 90: No images or unsupported format\n",
      "Page 91: No images or unsupported format\n",
      "Page 92: No images or unsupported format\n",
      "Page 93: No images or unsupported format\n"
     ]
    }
   ],
   "source": [
    "# Modify the file paths as needed\n",
    "file_paths = [\n",
    "    \"/content/Chicago Paper.pdf\",\n",
    "    \"/content/MAS thesis.pdf\"\n",
    "]\n",
    "\n",
    "file_text = read_files(file_paths)\n",
    "sentences, embeddings = create_embeddings(file_text, sentence_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dK78Mh3pD5P5",
    "outputId": "ae410fe0-f604-423d-e149-52886b462aee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity Scores:\n",
      "[0.16548398 0.03780279 0.3223809  ... 0.07290286 0.00178354 0.16407873]\n"
     ]
    }
   ],
   "source": [
    "# Calculate cosine similarity between the query and sentences\n",
    "similarity_scores = calculate_cosine_similarity(prompt_input, embeddings, sentence_model)\n",
    "print(f\"Cosine Similarity Scores:\\n{similarity_scores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xHyOLFvaD7-Q"
   },
   "outputs": [],
   "source": [
    "# Retrieve relevant context and generate a response\n",
    "context = search_context(prompt_input, sentences, embeddings, sentence_model, top_k=5)\n",
    "response = generate_llama2_response(prompt_input, \" \".join(context), pre_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5_ksWKDaIXVp"
   },
   "source": [
    "# ðŸš© Insert your prompt in prompt_input and run the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "YiKpXWzDIiN-",
    "outputId": "e821c1fa-e55a-4c20-b4a2-ce7935d27bc0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant:\n",
      " Sure, I'd be happy to help! Based on the provided document, the MAS thesis appears to be about the Macro Analyst System (MAS), which is a systematic approach to financial analysis that involves several stages, including financial modeling, sentiment analysis, risk assessment, and scenario analysis. The thesis also discusses common errors encountered in the MAS and their origins and impacts. Additionally, the thesis highlights the core principles of the MAS, which include autonomy, local views, decentralization, and cooperation. Is there anything specific you would like to know about the MAS thesis?\n"
     ]
    }
   ],
   "source": [
    "# Prompt\n",
    "prompt_input = \"What is the MAS thesis about?\"\n",
    "response = handle_query(prompt_input)\n",
    "\n",
    "print(f\"Assistant:\\n{response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eN4Qxa7yPDir"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
