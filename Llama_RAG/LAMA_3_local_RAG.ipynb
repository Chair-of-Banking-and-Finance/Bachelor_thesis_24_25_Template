{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2xcXUXrucu74",
      "metadata": {
        "id": "2xcXUXrucu74"
      },
      "source": [
        "# Setting up Google Colab and Hugging Face API\n",
        "\n",
        "Open this notebook in [colab](https://colab.research.google.com/github/Chair-of-Banking-and-Finance/Bachelor_thesis_24_25_Template/blob/main/Llama_RAG/LAMA_3_local_RAG.ipynb).\n",
        "\n",
        "## Getting a Hugging Face API Token\n",
        "1. **Create a Hugging Face account**: Go to [Hugging Face](https://huggingface.co/) and create an account if you don’t already have one.\n",
        "2. **Generate an API Token**: After logging in, click on your profile icon in the top right corner, and go to \"Settings\".\n",
        "3. **Access Tokens**: On the settings page, navigate to the \"Access Tokens\" tab.\n",
        "4. **Create a new token**: Click on \"New Token\", give it a name, and set the role to \"write\". This token will be used to authenticate and download models.\n",
        "5. **Copy the Token**: Copy the generated token and replace the `Hugging_Face_Token` variable in the script with your token.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "m74dgmwyckjo",
      "metadata": {
        "id": "m74dgmwyckjo"
      },
      "outputs": [],
      "source": [
    "Hugging_Face_Token = \"hf_XXXXXXXXXXXXXXXXXXXXXXXXXXXXX\"\n",
    "from huggingface_hub import login\n",
    "login(token=Hugging_Face_Token)"
  ]
},
    {
      "cell_type": "markdown",
      "source": [
        "Visit [Hugging Face's model page for Llama 2](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) and request access to the model."
      ],
      "metadata": {
        "id": "pbDCCz37XwAY"
      },
      "id": "pbDCCz37XwAY"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Required Libraries"
      ],
      "metadata": {
        "id": "ax0Bg7E1hdSE"
      },
      "id": "ax0Bg7E1hdSE"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78d0a4ca-1ce7-4a92-859f-948504ae0c8a",
      "metadata": {
        "id": "78d0a4ca-1ce7-4a92-859f-948504ae0c8a"
      },
      "outputs": [],
      "source": [
        "!pip install faiss-gpu\n",
        "!pip install -U bitsandbytes"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "fODkrei7hhMD"
      },
      "id": "fODkrei7hhMD"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import numpy as np\n",
        "import os"
      ],
      "metadata": {
        "id": "oHLGBZmWhjlB"
      },
      "id": "oHLGBZmWhjlB",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LlamaRAGPipeline:\n",
        "    def __init__(self,\n",
        "                 model_name='meta-llama/Llama-2-7b-chat-hf',\n",
        "                 embedding_model='BAAI/bge-small-en-v1.5'):\n",
        "        \"\"\"\n",
        "        Initialize Llama RAG Pipeline\n",
        "\n",
        "        Args:\n",
        "            model_name (str): Hugging Face Llama model path\n",
        "            embedding_model (str): Sentence transformer for embeddings\n",
        "        \"\"\"\n",
        "        # Set the Hugging Face token as an environment variable\n",
        "        hf_token = Hugging_Face_Token\n",
        "\n",
        "        # Quantization configuration for memory efficiency\n",
        "        quantization_config = BitsAndBytesConfig(\n",
        "              load_in_8bit=True  # Only include valid arguments\n",
        "        )\n",
        "\n",
        "        # Initialize tokenizer and model\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
        "            model_name,\n",
        "            use_fast=True,\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            device_map='auto',\n",
        "            torch_dtype=torch.float16,\n",
        "            quantization_config=quantization_config\n",
        "        )\n",
        "\n",
        "        # Initialize embedding model\n",
        "        self.embedding_model = SentenceTransformer(embedding_model)\n",
        "\n",
        "        # Initialize vector store\n",
        "        self.vector_store = None\n",
        "        self.documents = []\n",
        "\n",
        "    def add_documents(self, documents):\n",
        "        \"\"\"\n",
        "        Add documents to the RAG system\n",
        "\n",
        "        Args:\n",
        "            documents (list): List of text documents\n",
        "        \"\"\"\n",
        "        self.documents.extend(documents)\n",
        "\n",
        "        # Create embeddings\n",
        "        embeddings = self.embedding_model.encode(documents)\n",
        "\n",
        "        # Create FAISS index\n",
        "        dimension = embeddings.shape[1]\n",
        "        index = faiss.IndexFlatL2(dimension)\n",
        "        index.add(embeddings)\n",
        "\n",
        "        self.vector_store = index\n",
        "\n",
        "    def retrieve_context(self, query, top_k=3):\n",
        "        \"\"\"\n",
        "        Retrieve most relevant documents\n",
        "\n",
        "        Args:\n",
        "            query (str): Search query\n",
        "            top_k (int): Number of documents to retrieve\n",
        "\n",
        "        Returns:\n",
        "            list: Most relevant documents\n",
        "        \"\"\"\n",
        "        # Embed query\n",
        "        query_embedding = self.embedding_model.encode([query])\n",
        "\n",
        "        # Search vector store\n",
        "        distances, indices = self.vector_store.search(query_embedding, top_k)\n",
        "\n",
        "        # Retrieve documents\n",
        "        retrieved_docs = [self.documents[i] for i in indices[0]]\n",
        "        return retrieved_docs\n",
        "\n",
        "    def format_prompt(self, query, context):\n",
        "        \"\"\"\n",
        "        Format prompt for Llama with retrieved context\n",
        "\n",
        "        Args:\n",
        "            query (str): User query\n",
        "            context (list): Retrieved context documents\n",
        "\n",
        "        Returns:\n",
        "            str: Formatted prompt\n",
        "        \"\"\"\n",
        "        context_str = \"\\n\".join(context)\n",
        "        prompt = f\"\"\"[INST]\n",
        "        Context: {context_str}\n",
        "\n",
        "        Question: {query}\n",
        "\n",
        "        Based on the context, provide a comprehensive and precise answer. [/INST]\"\"\"\n",
        "\n",
        "        return prompt\n",
        "\n",
        "    def generate_response(self, prompt):\n",
        "        \"\"\"\n",
        "        Generate response using Llama model\n",
        "\n",
        "        Args:\n",
        "            prompt (str): Formatted prompt\n",
        "\n",
        "        Returns:\n",
        "            str: Generated response\n",
        "        \"\"\"\n",
        "        # Tokenize input\n",
        "        input_ids = self.tokenizer(prompt, return_tensors=\"pt\").input_ids.to(self.model.device)\n",
        "\n",
        "        # Generate response\n",
        "        output_ids = self.model.generate(\n",
        "            input_ids,\n",
        "            max_length=300,\n",
        "            num_return_sequences=1,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            do_sample=True\n",
        "        )\n",
        "\n",
        "        # Decode response\n",
        "        response = self.tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "        return response\n",
        "\n",
        "    def rag_pipeline(self, query):\n",
        "        \"\"\"\n",
        "        Complete RAG pipeline\n",
        "\n",
        "        Args:\n",
        "            query (str): User query\n",
        "\n",
        "        Returns:\n",
        "            str: Generated response\n",
        "        \"\"\"\n",
        "        # Retrieve context\n",
        "        context = self.retrieve_context(query)\n",
        "\n",
        "        # Format prompt\n",
        "        prompt = self.format_prompt(query, context)\n",
        "\n",
        "        # Generate response\n",
        "        response = self.generate_response(prompt)\n",
        "\n",
        "        return response"
      ],
      "metadata": {
        "id": "Mp_LJLsiZ-W_"
      },
      "id": "Mp_LJLsiZ-W_",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the text to be written to the file\n",
        "roman_empire_text = \"\"\"\n",
        "The Roman Empire: An Overview\n",
        "The Roman Empire was one of the most influential civilizations in human history, spanning over a millennium and leaving a legacy that shaped the world in areas such as governance, architecture, engineering, and law. Officially beginning in 27 BCE with the rise of Augustus Caesar, Rome transitioned from a republic to an empire, dominating vast territories that stretched from Britain in the northwest to Egypt in the southeast.\n",
        "\n",
        "Formation and Expansion\n",
        "The Roman Empire's foundation was built on centuries of conquest during the Roman Republic. Under Augustus, the empire ushered in a period of peace and stability known as the Pax Romana (Roman Peace), lasting about 200 years. During this time, Rome expanded its borders, solidifying control over Europe, North Africa, and parts of the Middle East.\n",
        "\n",
        "The empire was characterized by a vast network of cities connected by advanced roads and aqueducts, facilitating trade, military movements, and cultural exchange. Notable conquests include Gaul (modern-day France) under Julius Caesar, the annexation of Egypt after Cleopatra's defeat, and the consolidation of power in regions such as Spain and the Balkans.\n",
        "\n",
        "Culture and Society\n",
        "Roman society was highly stratified, with a clear distinction between the elite patricians, common plebeians, and enslaved individuals. Roman culture blended Latin traditions with influences from Greece and the regions it conquered. This fusion led to remarkable achievements in literature (Virgil’s Aeneid), philosophy (Cicero, Seneca), and architecture (the Colosseum, aqueducts, and the Pantheon).\n",
        "\n",
        "The Roman Empire was also a melting pot of religions. Initially polytheistic, it later became a cradle for Christianity, with Emperor Constantine legalizing the faith in 313 CE and Emperor Theodosius I declaring it the state religion by 380 CE.\n",
        "\n",
        "Governance and Law\n",
        "Rome was renowned for its administrative prowess and legal systems. The empire was divided into provinces, each governed by an appointed official. Roman law, codified in the Twelve Tables and later expanded, formed the foundation for many modern legal systems. Concepts like innocent until proven guilty and legal representation have their roots in Roman jurisprudence.\n",
        "\n",
        "Decline and Fall\n",
        "The decline of the Roman Empire was a gradual process influenced by internal and external factors. Political instability, economic struggles, and military overreach weakened the empire. The division of the empire into Eastern and Western halves in 395 CE further strained its cohesion. While the Western Roman Empire fell in 476 CE after being overrun by Germanic tribes, the Eastern Roman Empire, known as the Byzantine Empire, endured for another thousand years until the fall of Constantinople in 1453.\n",
        "\n",
        "Legacy\n",
        "The Roman Empire profoundly shaped Western civilization. Its contributions to governance, infrastructure, and culture remain influential today. Latin, the language of Rome, evolved into the Romance languages (Italian, French, Spanish, etc.), and Roman architecture inspired countless generations. The very concept of a republic and the rule of law owe much to Rome’s enduring influence.\n",
        "\n",
        "In essence, the Roman Empire stands as a testament to humanity’s capacity for organization, innovation, and adaptation, making it a cornerstone of global history.\n",
        "\"\"\"\n",
        "\n",
        "# Specify the directory and file name\n",
        "output_dir = \"./data\"\n",
        "file_name = \"roman_empire_overview.txt\"\n",
        "file_path = os.path.join(output_dir, file_name)\n",
        "\n",
        "# Ensure the output directory exists; if not, create it\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Write the text to the file with UTF-8 encoding\n",
        "with open(file_path, 'w', encoding='utf-8') as file:\n",
        "    file.write(roman_empire_text)"
      ],
      "metadata": {
        "id": "I-X41Xi9g6A4"
      },
      "id": "I-X41Xi9g6A4",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample documents about Roman Empire\n",
        "documents = [\"data/roman_empire_overview.txt\"]\n",
        "\n",
        "# Initialize RAG pipeline\n",
        "rag = LlamaRAGPipeline()\n",
        "\n",
        "# Add documents\n",
        "rag.add_documents(documents)\n",
        "\n",
        "# Example queries\n",
        "queries = [\n",
        "    \"Tell me about the founding of the Roman Empire\",\n",
        "    \"What made Roman engineering significant?\",\n",
        "    \"How extensive was the Roman Empire's territory?\"\n",
        "]\n",
        "\n",
        "# Run queries\n",
        "for query in queries:\n",
        "    print(f\"Query: {query}\")\n",
        "    response = rag.rag_pipeline(query)\n",
        "    print(f\"Response: {response}\\n\")\n"
      ],
      "metadata": {
        "id": "x_gnBpZng8ty"
      },
      "id": "x_gnBpZng8ty",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
