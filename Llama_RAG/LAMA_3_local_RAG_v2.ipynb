{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2xcXUXrucu74",
      "metadata": {
        "id": "2xcXUXrucu74"
      },
      "source": [
        "# Setting up Google Colab and Hugging Face API\n",
        "\n",
        "Open this notebook in [colab](https://colab.research.google.com/github/Chair-of-Banking-and-Finance/Bachelor_thesis_24_25_Template/blob/main/Llama_RAG/LAMA_3_local_RAG_v2.ipynb).\n",
        "\n",
        "## Getting a Hugging Face API Token\n",
        "1. **Create a Hugging Face account**: Go to [Hugging Face](https://huggingface.co/) and create an account if you donâ€™t already have one.\n",
        "2. **Generate an API Token**: After logging in, click on your profile icon in the top right corner, and go to \"Settings\".\n",
        "3. **Access Tokens**: On the settings page, navigate to the \"Access Tokens\" tab.\n",
        "4. **Create a new token**: Click on \"New Token\", give it a name, and set the role to \"write\". This token will be used to authenticate and download models.\n",
        "5. **Copy the Token**: Copy the generated token and replace the `Hugging_Face_Token` variable in the script with your token.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "m74dgmwyckjo",
      "metadata": {
        "id": "m74dgmwyckjo"
      },
      "outputs": [],
      "source": [
        "Hugging_face_token = \"hf_XXXXXXXXXXXXXXXXXXXXXXXXXX\"\n",
        "from huggingface_hub import login\n",
        "login(token=Hugging_face_token)\n",
        "import os\n",
        "\n",
        "os.environ[\"HF_TOKEN\"] = Hugging_face_token\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pbDCCz37XwAY",
      "metadata": {
        "id": "pbDCCz37XwAY"
      },
      "source": [
        "Visit [Hugging Face's model page for Llama 2](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) and request access to the model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ax0Bg7E1hdSE",
      "metadata": {
        "id": "ax0Bg7E1hdSE"
      },
      "source": [
        "### Install Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "78d0a4ca-1ce7-4a92-859f-948504ae0c8a",
      "metadata": {
        "collapsed": true,
        "id": "78d0a4ca-1ce7-4a92-859f-948504ae0c8a"
      },
      "outputs": [],
      "source": [
        "# Install basic dependencies\n",
        "!pip install -q transformers\n",
        "!pip install -q sentence-transformers\n",
        "!pip install -q torch\n",
        "!pip install -q PyPDF2\n",
        "!pip install -q tqdm\n",
        "!pip install -q hnswlib\n",
        "!pip install -q bitsandbytes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fODkrei7hhMD",
      "metadata": {
        "id": "fODkrei7hhMD"
      },
      "source": [
        "### Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "oHLGBZmWhjlB",
      "metadata": {
        "id": "oHLGBZmWhjlB"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import os\n",
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "import numpy as np\n",
        "import hnswlib\n",
        "from typing import List, Tuple, Optional\n",
        "from dataclasses import dataclass\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from PyPDF2 import PdfReader\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger(__name__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "Mp_LJLsiZ-W_",
      "metadata": {
        "id": "Mp_LJLsiZ-W_"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class RetrievedDocument:\n",
        "    \"\"\"Data class for storing retrieved documents and their metadata.\"\"\"\n",
        "    content: str\n",
        "    similarity_score: float\n",
        "    source: str = \"\"\n",
        "\n",
        "class DocumentProcessor:\n",
        "    \"\"\"Handles document loading and preprocessing.\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def load_document(file_path: str) -> Optional[str]:\n",
        "        \"\"\"Load document content from various file formats.\"\"\"\n",
        "        try:\n",
        "            if file_path.endswith('.pdf'):\n",
        "                with open(file_path, 'rb') as file:\n",
        "                    reader = PdfReader(file)\n",
        "                    return ' '.join(page.extract_text() for page in reader.pages)\n",
        "\n",
        "            elif file_path.endswith('.txt'):\n",
        "                with open(file_path, 'r', encoding='utf-8') as file:\n",
        "                    return file.read()\n",
        "            else:\n",
        "                logger.warning(f\"Unsupported file format: {file_path}\")\n",
        "                return None\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error loading document {file_path}: {e}\")\n",
        "            return None\n",
        "\n",
        "class HNSWRetriever:\n",
        "    \"\"\"Document retrieval system using HNSWLib for efficient similarity search.\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 embedding_model: str = 'BAAI/bge-small-en-v1.5',\n",
        "                 space: str = 'cosine',\n",
        "                 ef_construction: int = 200,\n",
        "                 M: int = 16):\n",
        "        \"\"\"\n",
        "        Initialize the retriever with HNSWLib index.\n",
        "\n",
        "        Args:\n",
        "            embedding_model: Name of the embedding model\n",
        "            space: Distance metric ('cosine', 'l2', 'ip')\n",
        "            ef_construction: Number of neighbors to consider during index construction\n",
        "            M: Number of bi-directional links created for every new element\n",
        "        \"\"\"\n",
        "        self.embedding_model = SentenceTransformer(embedding_model)\n",
        "        self.documents = []\n",
        "        self.document_sources = []\n",
        "        self.index = None\n",
        "        self.space = space\n",
        "        self.ef_construction = ef_construction\n",
        "        self.M = M\n",
        "\n",
        "    def add_documents(self, documents: List[str], sources: List[str] = None) -> None:\n",
        "        \"\"\"Add documents to the retrieval system.\"\"\"\n",
        "        if not documents:\n",
        "            logger.warning(\"No documents provided for indexing\")\n",
        "            return\n",
        "\n",
        "        logger.info(f\"Adding {len(documents)} documents to the index\")\n",
        "\n",
        "        # Store documents and their sources\n",
        "        start_idx = len(self.documents)\n",
        "        self.documents.extend(documents)\n",
        "        if sources:\n",
        "            self.document_sources.extend(sources)\n",
        "\n",
        "        # Create embeddings\n",
        "        embeddings = self.embedding_model.encode(\n",
        "            documents,\n",
        "            show_progress_bar=True,\n",
        "            batch_size=32\n",
        "        )\n",
        "\n",
        "        # Initialize or update HNSWLib index\n",
        "        if self.index is None:\n",
        "            dimension = embeddings.shape[1]\n",
        "            self.index = hnswlib.Index(space=self.space, dim=dimension)\n",
        "            self.index.init_index(\n",
        "                max_elements=len(documents) * 2,  # Allow for future additions\n",
        "                ef_construction=self.ef_construction,\n",
        "                M=self.M\n",
        "            )\n",
        "            self.index.add_items(embeddings, list(range(len(documents))))\n",
        "        else:\n",
        "            self.index.resize_index(len(self.documents))\n",
        "            self.index.add_items(\n",
        "                embeddings,\n",
        "                list(range(start_idx, start_idx + len(documents)))\n",
        "            )\n",
        "\n",
        "        logger.info(\"Documents successfully indexed\")\n",
        "\n",
        "    def retrieve(self, query: str, top_k: int = 3) -> List[RetrievedDocument]:\n",
        "        \"\"\"Retrieve most relevant documents for a query.\"\"\"\n",
        "        if not self.documents:\n",
        "            logger.warning(\"No documents in the index\")\n",
        "            return []\n",
        "\n",
        "        # Encode query\n",
        "        query_embedding = self.embedding_model.encode([query])\n",
        "\n",
        "        # Search index\n",
        "        try:\n",
        "            # Get nearest neighbors\n",
        "            labels, distances = self.index.knn_query(query_embedding, k=min(top_k, len(self.documents)))\n",
        "\n",
        "            # Convert distances to similarities if using cosine space\n",
        "            if self.space == 'cosine':\n",
        "                similarities = 1 - distances[0]\n",
        "            else:\n",
        "                similarities = -distances[0]  # Convert distance to similarity\n",
        "\n",
        "            # Package results\n",
        "            results = []\n",
        "            for idx, similarity in zip(labels[0], similarities):\n",
        "                source = self.document_sources[idx] if self.document_sources else \"\"\n",
        "                results.append(RetrievedDocument(\n",
        "                    content=self.documents[idx],\n",
        "                    similarity_score=float(similarity),\n",
        "                    source=source\n",
        "                ))\n",
        "\n",
        "            return results\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error during retrieval: {e}\")\n",
        "            return []\n",
        "\n",
        "class HNSWRAGPipeline:\n",
        "    \"\"\"RAG pipeline using HNSWLib for retrieval.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_name: str = 'meta-llama/Llama-2-7b-chat-hf',\n",
        "        embedding_model: str = 'BAAI/bge-small-en-v1.5',\n",
        "        device: str = 'auto',\n",
        "        load_in_4bit: bool = True\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize the RAG pipeline.\n",
        "\n",
        "        Args:\n",
        "            model_name: Name of the language model to use\n",
        "            embedding_model: Name of the embedding model\n",
        "            device: Device to use ('auto', 'cuda', 'cpu')\n",
        "            load_in_4bit: Whether to load model in 4-bit precision\n",
        "        \"\"\"\n",
        "        logger.info(f\"Initializing HNSWRAGPipeline with model: {model_name}\")\n",
        "\n",
        "        # Initialize retriever\n",
        "        self.retriever = HNSWRetriever(embedding_model)\n",
        "\n",
        "        # Initialize tokenizer and model\n",
        "        try:\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
        "                model_name,\n",
        "                use_fast=True,\n",
        "                trust_remote_code=True\n",
        "            )\n",
        "\n",
        "            if device == 'auto':\n",
        "                device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "            # Configure model loading based on available resources\n",
        "            model_kwargs = {\n",
        "                \"device_map\": device,\n",
        "                \"torch_dtype\": torch.float16 if device == 'cuda' else torch.float32,\n",
        "                \"low_cpu_mem_usage\": True\n",
        "            }\n",
        "\n",
        "            if device == 'cuda' and load_in_4bit:\n",
        "                try:\n",
        "                    from transformers import BitsAndBytesConfig\n",
        "\n",
        "                    model_kwargs.update({\n",
        "                        \"quantization_config\": BitsAndBytesConfig(\n",
        "                            load_in_4bit=True,\n",
        "                            bnb_4bit_compute_dtype=torch.float16,\n",
        "                            bnb_4bit_use_double_quant=True,\n",
        "                            bnb_4bit_quant_type=\"nf4\"\n",
        "                        )\n",
        "                    })\n",
        "                except ImportError:\n",
        "                    logger.warning(\"bitsandbytes not available, falling back to 16-bit\")\n",
        "\n",
        "            self.model = AutoModelForCausalLM.from_pretrained(\n",
        "                model_name,\n",
        "                **model_kwargs\n",
        "            )\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error loading model: {e}\")\n",
        "            raise\n",
        "\n",
        "        logger.info(\"Pipeline initialized successfully\")\n",
        "\n",
        "    def add_documents(self, file_paths: List[str]) -> None:\n",
        "        \"\"\"Add documents to the retrieval system.\"\"\"\n",
        "        documents = []\n",
        "        valid_sources = []\n",
        "\n",
        "        for path in tqdm(file_paths, desc=\"Loading documents\"):\n",
        "            content = DocumentProcessor.load_document(path)\n",
        "            if content:\n",
        "                documents.append(content)\n",
        "                valid_sources.append(path)\n",
        "\n",
        "        self.retriever.add_documents(documents, valid_sources)\n",
        "\n",
        "    def _format_prompt(self, query: str, retrieved_docs: List[RetrievedDocument]) -> str:\n",
        "        \"\"\"Format the prompt with retrieved context.\"\"\"\n",
        "        context_str = \"\\n\\n\".join(\n",
        "            f\"[Document {i+1} (Relevance: {doc.similarity_score:.2f})]\\n{doc.content}\"\n",
        "            for i, doc in enumerate(retrieved_docs)\n",
        "        )\n",
        "\n",
        "        return f\"\"\"[INST]\n",
        "Using the following retrieved documents as context, please answer the question.\n",
        "If the context doesn't contain relevant information, use your general knowledge\n",
        "but indicate this in your response.\n",
        "\n",
        "Context:\n",
        "{context_str}\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Please provide a comprehensive and accurate answer based on the provided context.\n",
        "If the context is insufficient, indicate what information comes from your general knowledge.[/INST]\"\"\"\n",
        "\n",
        "    def generate_response(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        max_new_tokens: int = 512,\n",
        "        temperature: float = 0.7,\n",
        "        top_p: float = 0.9\n",
        "    ) -> str:\n",
        "        \"\"\"Generate a response using the language model.\"\"\"\n",
        "        try:\n",
        "            inputs = self.tokenizer(\n",
        "                prompt,\n",
        "                return_tensors=\"pt\",\n",
        "                truncation=True,\n",
        "                max_length=2048\n",
        "            ).to(self.model.device)\n",
        "\n",
        "            outputs = self.model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=max_new_tokens,\n",
        "                temperature=temperature,\n",
        "                top_p=top_p,\n",
        "                do_sample=True,\n",
        "                pad_token_id=self.tokenizer.eos_token_id\n",
        "            )\n",
        "\n",
        "            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "            response = response.split(\"[/INST]\")[-1].strip()\n",
        "\n",
        "            return response\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error generating response: {e}\")\n",
        "            return \"I apologize, but I encountered an error while generating the response.\"\n",
        "\n",
        "    def query(\n",
        "        self,\n",
        "        query: str,\n",
        "        top_k: int = 3,\n",
        "        max_new_tokens: int = 512\n",
        "    ) -> Tuple[str, List[RetrievedDocument]]:\n",
        "        \"\"\"Process a query through the complete RAG pipeline.\"\"\"\n",
        "        logger.info(f\"Processing query: {query}\")\n",
        "\n",
        "        # Retrieve relevant documents\n",
        "        retrieved_docs = self.retriever.retrieve(query, top_k=top_k)\n",
        "\n",
        "        if not retrieved_docs:\n",
        "            logger.warning(\"No relevant documents found\")\n",
        "            prompt = f\"\"\"[INST]Please answer this question using your general knowledge:\n",
        "{query}[/INST]\"\"\"\n",
        "        else:\n",
        "            prompt = self._format_prompt(query, retrieved_docs)\n",
        "\n",
        "        # Generate response\n",
        "        response = self.generate_response(prompt, max_new_tokens=max_new_tokens)\n",
        "\n",
        "        return response, retrieved_docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "I-X41Xi9g6A4",
      "metadata": {
        "id": "I-X41Xi9g6A4"
      },
      "outputs": [],
      "source": [
        "# DELETE IF YOU WORK WITH THE REAL DATA, ONLY AN EXAMPLE\n",
        "# Define the text to be written to the file\n",
        "roman_empire_text = \"\"\"\n",
        "The Roman Empire: An Overview\n",
        "The Roman Empire was one of the most influential civilizations in human history, spanning over a millennium and leaving a legacy that shaped the world in areas such as governance, architecture, engineering, and law. Officially beginning in 27 BCE with the rise of Augustus Caesar, Rome transitioned from a republic to an empire, dominating vast territories that stretched from Britain in the northwest to Egypt in the southeast.\n",
        "\n",
        "Formation and Expansion\n",
        "The Roman Empire's foundation was built on centuries of conquest during the Roman Republic. Under Augustus, the empire ushered in a period of peace and stability known as the Pax Romana (Roman Peace), lasting about 200 years. During this time, Rome expanded its borders, solidifying control over Europe, North Africa, and parts of the Middle East.\n",
        "\n",
        "The empire was characterized by a vast network of cities connected by advanced roads and aqueducts, facilitating trade, military movements, and cultural exchange. Notable conquests include Gaul (modern-day France) under Julius Caesar, the annexation of Egypt after Cleopatra's defeat, and the consolidation of power in regions such as Spain and the Balkans.\n",
        "\n",
        "Culture and Society\n",
        "Roman society was highly stratified, with a clear distinction between the elite patricians, common plebeians, and enslaved individuals. Roman culture blended Latin traditions with influences from Greece and the regions it conquered. This fusion led to remarkable achievements in literature (Virgilâ€™s Aeneid), philosophy (Cicero, Seneca), and architecture (the Colosseum, aqueducts, and the Pantheon).\n",
        "\n",
        "The Roman Empire was also a melting pot of religions. Initially polytheistic, it later became a cradle for Christianity, with Emperor Constantine legalizing the faith in 313 CE and Emperor Theodosius I declaring it the state religion by 380 CE.\n",
        "\n",
        "Governance and Law\n",
        "Rome was renowned for its administrative prowess and legal systems. The empire was divided into provinces, each governed by an appointed official. Roman law, codified in the Twelve Tables and later expanded, formed the foundation for many modern legal systems. Concepts like innocent until proven guilty and legal representation have their roots in Roman jurisprudence.\n",
        "\n",
        "Decline and Fall\n",
        "The decline of the Roman Empire was a gradual process influenced by internal and external factors. Political instability, economic struggles, and military overreach weakened the empire. The division of the empire into Eastern and Western halves in 395 CE further strained its cohesion. While the Western Roman Empire fell in 476 CE after being overrun by Germanic tribes, the Eastern Roman Empire, known as the Byzantine Empire, endured for another thousand years until the fall of Constantinople in 1453.\n",
        "\n",
        "Legacy\n",
        "The Roman Empire profoundly shaped Western civilization. Its contributions to governance, infrastructure, and culture remain influential today. Latin, the language of Rome, evolved into the Romance languages (Italian, French, Spanish, etc.), and Roman architecture inspired countless generations. The very concept of a republic and the rule of law owe much to Romeâ€™s enduring influence.\n",
        "\n",
        "In essence, the Roman Empire stands as a testament to humanityâ€™s capacity for organization, innovation, and adaptation, making it a cornerstone of global history.\n",
        "\"\"\"\n",
        "\n",
        "# # Specify the directory and file name\n",
        "# output_dir = \"./data\"\n",
        "# file_name = \"roman_empire_overview.txt\"\n",
        "# file_path = os.path.join(output_dir, file_name)\n",
        "\n",
        "# # Ensure the output directory exists; if not, create it\n",
        "# os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# # Write the text to the file with UTF-8 encoding\n",
        "# with open(file_path, 'w', encoding='utf-8') as file:\n",
        "#     file.write(roman_empire_text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the directory and file name\n",
        "output_dir = \"./data\"\n",
        "#file_name = \"roman_empire_overview.txt\"\n",
        "file_path = os.path.join(output_dir, file_name)\n",
        "\n",
        "# Ensure the output directory exists; if not, create it\n",
        "os.makedirs(output_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "RA5PTzTeIaaB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "outputId": "6ef5b086-4a21-4cf9-94e2-703b0da25ac7"
      },
      "id": "RA5PTzTeIaaB",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'file_name' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-c23c447bb37d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0moutput_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"./data\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#file_name = \"roman_empire_overview.txt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Ensure the output directory exists; if not, create it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'file_name' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DELETE IF YOU WORK WITH THE REAL DATA, ONLY AN EXAMPLE\n",
        "# Write the text to the file with UTF-8 encoding\n",
        "with open(file_path, 'w', encoding='utf-8') as file:\n",
        "    file.write(roman_empire_text)"
      ],
      "metadata": {
        "id": "IV4YuDmYIahk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "outputId": "76eaca8e-a669-40ef-cfc1-4d7630661f74"
      },
      "id": "IV4YuDmYIahk",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'file_path' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-369bb533940a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# DELETE IF YOU WORK WITH THE REAL DATA, ONLY AN EXAMPLE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Write the text to the file with UTF-8 encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroman_empire_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'file_path' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "4GU9cGpPNW52",
      "metadata": {
        "id": "4GU9cGpPNW52"
      },
      "outputs": [],
      "source": [
        "# First, clear any existing CUDA memory\n",
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "  torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "x_gnBpZng8ty",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158,
          "referenced_widgets": [
            "2dc210630bf340d19d48c039496e7b8d",
            "4b1562ef6d8d4679a46ce72b14e28c62",
            "88ba714eeb614538a57ac62f87f39e9c",
            "bca394a086f044fcb7d58427ed24f251",
            "1233da4372d04a02a4e17b120ad70c3a",
            "71d436ec636549b5ba624da881faab81",
            "623d7e613c804c2e91dd7744705167f7",
            "2c1ee018ae8c42469ee4767eb024cddf",
            "b0373fadf1214b83baf4e934067cbde0",
            "04a480a9bf4945369f544bda9ae20ff0",
            "239bf5450ed04c23bbb503c5be0805bb"
          ]
        },
        "id": "x_gnBpZng8ty",
        "outputId": "760fbb65-1a86-4e83-884a-e83021814977"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2dc210630bf340d19d48c039496e7b8d"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Initialize pipeline forcing CPU mode for all components\n",
        "rag = HNSWRAGPipeline(\n",
        "    model_name='meta-llama/Llama-2-7b-chat-hf'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Document directory\n",
        "document_dir = \"data\"\n",
        "\n",
        "# Get document files\n",
        "document_files = [\n",
        "    os.path.join(document_dir, f)\n",
        "    for f in os.listdir(document_dir)\n",
        "    if os.path.isfile(os.path.join(document_dir, f))\n",
        "]\n",
        "\n",
        "# Add documents to RAG\n",
        "rag.add_documents(document_files)\n",
        "\n",
        "# Example query\n",
        "query = \"What were the major achievements of the Roman Empire?\"\n",
        "#query = \"What is the driver of the upgrade of BMW's Ebit earnings estimates on average FY23/24?\"\n",
        "\n",
        "# Query the RAG system\n",
        "response, retrieved_docs = rag.query(query)\n",
        "\n",
        "# Function to limit and process context\n",
        "def process_context(retrieved_docs, limit=1000):\n",
        "    \"\"\"\n",
        "    Extracts and limits the context to the most relevant parts,\n",
        "    sorting documents by similarity in descending order.\n",
        "    \"\"\"\n",
        "    # Sort documents by similarity score in descending order\n",
        "    docs_sorted = sorted(retrieved_docs, key=lambda d: d.similarity_score, reverse=True)\n",
        "\n",
        "    # Join the top portions of each document's content\n",
        "    context = \"\\n\\n\".join([\n",
        "        f\"[Similarity: {doc.similarity_score:.4f}]\\n{doc.content[:limit]}\"\n",
        "        for doc in docs_sorted\n",
        "    ])\n",
        "\n",
        "    return context\n",
        "\n",
        "# Generate a concise answer\n",
        "def generate_answer(query, retrieved_docs):\n",
        "    if retrieved_docs:\n",
        "        # Limit the context size to avoid overwhelming the model\n",
        "        context = process_context(retrieved_docs)\n",
        "\n",
        "        # Instructional prompt for generating a concise response\n",
        "        prompt = f\"\"\"\n",
        "        You are an AI assistant. Based on the provided context, answer the user's question in 1-2 sentences with a concise, clear, and specific response.\n",
        "        If the context does not contain the necessary information, provide an answer using general knowledge and mention that the context was insufficient.\n",
        "\n",
        "        Context:\n",
        "        {context}\n",
        "\n",
        "        Question:\n",
        "        {query}\n",
        "\n",
        "        Answer:\n",
        "        \"\"\"\n",
        "        # Generate a concise response using the RAG's language model\n",
        "        concise_response = rag.generate_response(prompt)\n",
        "        return concise_response.strip()\n",
        "    else:\n",
        "        return \"No relevant documents were retrieved to answer the query.\"\n",
        "\n",
        "# Generate and print the concise answer\n",
        "generated_response = generate_answer(query, retrieved_docs)\n",
        "\n",
        "print(f\"\\nQuery: {query}\")\n",
        "print(\"\\nGenerated Response:\")\n",
        "print(generated_response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507,
          "referenced_widgets": [
            "13aea3ee577a4f73bf875d82d86ceaa2",
            "efea12cd97f44b5cb666d9c8f3abab98",
            "f11eb36ce70a4a82b7e3f7571312fce6",
            "1962134578ed4a8796edad8b232a3f8d",
            "aaced5a595184fabbbd24c67c8423251",
            "b17ae9f8e1604e3894977fc125db8638",
            "829d848c6756407ca39b87780f416fb1",
            "4da2ade97cdd4000b4752d9ecc3fd6a1",
            "0ba3504bc463408ea4c64ba4d66dfda3",
            "259e4412ec6c451dbbd9eaf716fec6e7",
            "038ae36163d94372938d47100bfc9f73"
          ]
        },
        "id": "Xe7pAMxXFFco",
        "outputId": "3a600d26-e163-4180-f4e6-788d07d3f45c"
      },
      "id": "Xe7pAMxXFFco",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading documents: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 680.56it/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "13aea3ee577a4f73bf875d82d86ceaa2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Query: What were the major achievements of the Roman Empire?\n",
            "\n",
            "Generated Response:\n",
            "You are an AI assistant. Based on the provided context, answer the user's question in 1-2 sentences with a concise, clear, and specific response.\n",
            "        If the context does not contain the necessary information, provide an answer using general knowledge and mention that the context was insufficient.\n",
            "\n",
            "        Context:\n",
            "        [Similarity: 0.7649]\n",
            "\n",
            "The Roman Empire: An Overview\n",
            "The Roman Empire was one of the most influential civilizations in human history, spanning over a millennium and leaving a legacy that shaped the world in areas such as governance, architecture, engineering, and law. Officially beginning in 27 BCE with the rise of Augustus Caesar, Rome transitioned from a republic to an empire, dominating vast territories that stretched from Britain in the northwest to Egypt in the southeast.\n",
            "\n",
            "Formation and Expansion\n",
            "The Roman Empire's foundation was built on centuries of conquest during the Roman Republic. Under Augustus, the empire ushered in a period of peace and stability known as the Pax Romana (Roman Peace), lasting about 200 years. During this time, Rome expanded its borders, solidifying control over Europe, North Africa, and parts of the Middle East.\n",
            "\n",
            "The empire was characterized by a vast network of cities connected by advanced roads and aqueducts, facilitating trade, military movements, and cultural exchange. Not\n",
            "\n",
            "        Question:\n",
            "        What were the major achievements of the Roman Empire?\n",
            "\n",
            "        Answer:\n",
            "         The Roman Empire's major achievements include the establishment of a comprehensive system of governance, the development of advanced infrastructure such as roads and aqueducts, and the spread of Latin as a common language throughout its territories. Additionally, the empire made significant contributions to the fields of architecture, engineering, and law, shaping the course of Western civilization. However, the context does not provide enough information to provide a more specific answer.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LxkzKIKo3tyz"
      },
      "id": "LxkzKIKo3tyz",
      "execution_count": 12,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2dc210630bf340d19d48c039496e7b8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4b1562ef6d8d4679a46ce72b14e28c62",
              "IPY_MODEL_88ba714eeb614538a57ac62f87f39e9c",
              "IPY_MODEL_bca394a086f044fcb7d58427ed24f251"
            ],
            "layout": "IPY_MODEL_1233da4372d04a02a4e17b120ad70c3a"
          }
        },
        "4b1562ef6d8d4679a46ce72b14e28c62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_71d436ec636549b5ba624da881faab81",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_623d7e613c804c2e91dd7744705167f7",
            "value": "Loadingâ€‡checkpointâ€‡shards:â€‡100%"
          }
        },
        "88ba714eeb614538a57ac62f87f39e9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2c1ee018ae8c42469ee4767eb024cddf",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b0373fadf1214b83baf4e934067cbde0",
            "value": 2
          }
        },
        "bca394a086f044fcb7d58427ed24f251": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_04a480a9bf4945369f544bda9ae20ff0",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_239bf5450ed04c23bbb503c5be0805bb",
            "value": "â€‡2/2â€‡[01:08&lt;00:00,â€‡31.16s/it]"
          }
        },
        "1233da4372d04a02a4e17b120ad70c3a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "71d436ec636549b5ba624da881faab81": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "623d7e613c804c2e91dd7744705167f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2c1ee018ae8c42469ee4767eb024cddf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b0373fadf1214b83baf4e934067cbde0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "04a480a9bf4945369f544bda9ae20ff0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "239bf5450ed04c23bbb503c5be0805bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "13aea3ee577a4f73bf875d82d86ceaa2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_efea12cd97f44b5cb666d9c8f3abab98",
              "IPY_MODEL_f11eb36ce70a4a82b7e3f7571312fce6",
              "IPY_MODEL_1962134578ed4a8796edad8b232a3f8d"
            ],
            "layout": "IPY_MODEL_aaced5a595184fabbbd24c67c8423251"
          }
        },
        "efea12cd97f44b5cb666d9c8f3abab98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b17ae9f8e1604e3894977fc125db8638",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_829d848c6756407ca39b87780f416fb1",
            "value": "Batches:â€‡100%"
          }
        },
        "f11eb36ce70a4a82b7e3f7571312fce6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4da2ade97cdd4000b4752d9ecc3fd6a1",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0ba3504bc463408ea4c64ba4d66dfda3",
            "value": 1
          }
        },
        "1962134578ed4a8796edad8b232a3f8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_259e4412ec6c451dbbd9eaf716fec6e7",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_038ae36163d94372938d47100bfc9f73",
            "value": "â€‡1/1â€‡[00:00&lt;00:00,â€‡â€‡1.59it/s]"
          }
        },
        "aaced5a595184fabbbd24c67c8423251": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b17ae9f8e1604e3894977fc125db8638": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "829d848c6756407ca39b87780f416fb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4da2ade97cdd4000b4752d9ecc3fd6a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ba3504bc463408ea4c64ba4d66dfda3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "259e4412ec6c451dbbd9eaf716fec6e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "038ae36163d94372938d47100bfc9f73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}